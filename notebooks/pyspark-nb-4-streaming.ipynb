{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79b7995",
   "metadata": {},
   "source": [
    "# 4. Structured Streaming with the Retail Data-Set #\n",
    "Use the by-day retail data-set to simulate a daily feed of data to be read by a Spark Structured Streaming job.   \n",
    "\n",
    "Each file that is read in by the `spark.ReadStream` method uses the option *maxFilesPerTrigger=1* to trigger an update to the streaming dataframe contents.  \n",
    "  \n",
    "A transformation called `purchaseByCustomerPerHour` is mapped to the streaming dataframe with a query transformation to show the top 5 customers and their spend so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acfedfc0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/02 00:20:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-nb-4-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"4096m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4781e4e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Spark Streaming functions for creating window and defining a column (col) to calculate the window over\n",
    "#from pyspark.sql.functions import window, column, desc, col\n",
    "from pyspark.sql.functions import window,  col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f393f2",
   "metadata": {},
   "source": [
    "First, get a sample of the data to be processed in a continuous stream.  \n",
    "+ We have to assume that future data will have the same schema.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fb63d1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_sample = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2256c784",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='536365', StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, InvoiceDate='2010-12-01 08:26:00', UnitPrice=2.55, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='71053', Description='WHITE METAL LANTERN', Quantity=6, InvoiceDate='2010-12-01 08:26:00', UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84406B', Description='CREAM CUPID HEARTS COAT HANGER', Quantity=8, InvoiceDate='2010-12-01 08:26:00', UnitPrice=2.75, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84029G', Description='KNITTED UNION FLAG HOT WATER BOTTLE', Quantity=6, InvoiceDate='2010-12-01 08:26:00', UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='536365', StockCode='84029E', Description='RED WOOLLY HOTTIE WHITE HEART.', Quantity=6, InvoiceDate='2010-12-01 08:26:00', UnitPrice=3.39, CustomerID=17850.0, Country='United Kingdom')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f295b49",
   "metadata": {},
   "source": [
    "#### Create a Streaming Dataframe ####\n",
    "The Streaming Dataframe is created with a Schema based on the data sample we took earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996f08b5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get a Schema from the data_sample\n",
    "staticSchema = data_sample.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e48569",
   "metadata": {},
   "source": [
    "Create a streaming dataframe using `maxFilesPerTrigger` to trigger an update to the dataframe on file-by-file basis.  The `header` option caters for (assumes) headers on each file that is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c57b5c1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create a Streaming DataFrame based on a Static Schema from the data_sample\n",
    "streamingDataFrame = spark.readStream.schema(staticSchema).option(\"maxFilesPerTrigger\",1).format(\"csv\").option(\"header\", \"true\")\\\n",
    "       .load(\"/opt/workspace/datain/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d82892",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the stream status\n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e48df",
   "metadata": {},
   "source": [
    "A streaming transformation `purchaseByCustomerPerHour` is created below; This applies a dataframe transformation which groups the data by *CustomerID* and *InvoiceDate* and sums the \"total_cost\" of *UnitPrice* x *Quantity*.  \n",
    "\n",
    "The `window` function is used to define the size of window over which this summary is provided - in this case a 1-day window of time.  This means we get to see which customers have bought the most *in a single day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c49be3d5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create a transformation that sum's customer purchase per hour\n",
    "purchaseByCustomerPerHour = streamingDataFrame.selectExpr(\"CustomerID\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\") \\\n",
    "                                              .groupBy(col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\")) \\\n",
    "                                              .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2f8ed",
   "metadata": {},
   "source": [
    "Create an in-memory structure called `customer_purchases` that  is written to with the output from the streaming transformation. This can be queried using Spark SQL to see the latest result-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f559bfa",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 00:22:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b57575fe-3add-409c-999f-d726580a4772. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/02 00:22:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "[Stage 24:=============>                                         (49 + 8) / 200]\r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name customer_purchases as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4124/63015754.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate updates to an in-memory table after each trigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpurchaseByCustomerPerHour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customer_purchases\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name customer_purchases as a query with that name is already active in this SparkSession"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===========================>                          (100 + 8) / 200]\r"
     ]
    }
   ],
   "source": [
    "# Generate updates to an in-memory table after each trigger\n",
    "purchaseByCustomerPerHour.writeStream.format(\"memory\").queryName(\"customer_purchases\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69212da0",
   "metadata": {},
   "source": [
    "Query `customer_purchases` to see the latest view of which customers have spent the most in a single day.  The results change as new retail data day-files are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a0c50bf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:============================================>         (165 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   18102.0|{2010-12-07 00:00...|          25920.37|\n",
      "|      null|{2010-12-10 00:00...|25399.560000000012|\n",
      "|      null|{2010-12-06 00:00...|23395.099999999904|\n",
      "|      null|{2010-12-03 00:00...| 23021.99999999999|\n",
      "|      null|{2010-12-14 00:00...|15929.879999999974|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99fac5e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   18102.0|{2010-12-07 00:00...|          25920.37|\n",
      "|      null|{2010-12-10 00:00...|25399.560000000012|\n",
      "|      null|{2010-12-17 00:00...|25371.769999999768|\n",
      "|      null|{2010-12-06 00:00...|23395.099999999904|\n",
      "|      null|{2010-12-03 00:00...| 23021.99999999999|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=================================================>    (182 + 4) / 200]\r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d582e89",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e999c2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
